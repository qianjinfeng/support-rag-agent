# rag_engine.py
from langchain_ollama import OllamaEmbeddings, OllamaLLM
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
import os

# æ›´æ–°åçš„ rag_engine.py æ ¸å¿ƒéƒ¨åˆ†


# âœ… ä½¿ç”¨æ–°æ–¹å¼åˆ›å»º embedding
embedding_model = OllamaEmbeddings(
    model="nomic-embed-text",
    base_url="http://host.docker.internal:11434"
)

# âœ… ä½¿ç”¨ OllamaLLMï¼ˆæ–°ç±»ï¼‰
llm = OllamaLLM(
    model="llama3.2:1b",
    base_url="http://host.docker.internal:11434",
    temperature=0.2
)

# åç»­é€»è¾‘ä¸å˜...

PROMPT_TEMPLATE = """
You are a professional technical support assistant. Please analyze the user's issue and provide recommendations.

ã€Device Contextã€‘
- Software Version: {sw_version}
- Model Type: {model_type}
- Components: {components}
- Country: {country}

ã€Relevant Historical Casesã€‘
{context}

ã€User's Problemã€‘
"{question}"

Please respond in the following format:

ğŸ” **Problem Analysis**:
Briefly explain the possible root cause.

ğŸ› ï¸ **Recommended Solution**:
1. Step-by-step actions.
2. If multiple, list clearly.

ğŸ“Œ **Notes**:
- Remind to back up before updates.
- Mention compliance if needed.

ğŸ”— **Reference Cases**:
{references}

â“ **If More Info Needed**:
Suggest what logs or details to collect.
"""

class RAGEngine:
    def __init__(self, db_path="./chroma_db"):
        self.db_path = db_path
        self.vectorstore = None

    def load_data(self, data):
        """å¯¼å…¥å†å²é—®é¢˜æ•°æ®"""
        texts = []
        metadatas = []
        ids = []

        for item in data:
            text = f"{item['problem_description']} {item['root_cause']} {item['solution']}"
            texts.append(text)
            metadatas.append(item["metadata"])
            ids.append(item["id"])

        self.vectorstore = Chroma.from_texts(
            texts=texts,
            embedding=embedding_model,
            metadatas=metadatas,
            ids=ids,
            persist_directory=self.db_path
        )
        print(f"âœ… Loaded {len(data)} cases into vector DB")

    def query(self, question, sw_version=None, model_type=None, components=None, country=None):
        """æŸ¥è¯¢å¹¶ç”Ÿæˆå›ç­”"""
        if not self.vectorstore:
            self.vectorstore = Chroma(
                persist_directory=self.db_path,
                embedding_function=embedding_model
            )

        # æ„å»º filter ä½¿ç”¨ ChromaDB æ­£ç¡®çš„æ“ä½œç¬¦è¯­æ³•
        filters = []
        if model_type:
            filters.append({"model_type": {"$eq": model_type}})
        if sw_version:
            filters.append({"sw_version": {"$eq": sw_version}})
        # å¯é€‰ï¼šæ·»åŠ  components æˆ– country è¿‡æ»¤
        # if country:
        #     filters.append({"country": {"$eq": country}})

        # åªæœ‰å½“æœ‰è‡³å°‘ä¸€ä¸ªæ¡ä»¶æ—¶ï¼Œæ‰ä½¿ç”¨ $and
        if len(filters) == 0:
            # æ— è¿‡æ»¤æ¡ä»¶ï¼šä¸ä¼  filter
            kwargs = {"k": 3}
        elif len(filters) == 1:
            # å•ä¸ªæ¡ä»¶ï¼šç›´æ¥ä¼ ï¼ˆä¸éœ€è¦ $andï¼‰
            kwargs = {"k": 3, "filter": filters[0]}
        else:
            # å¤šä¸ªæ¡ä»¶ï¼šç”¨ $and åŒ…è£¹
            kwargs = {"k": 3, "filter": {"$and": filters}}

        results = self.vectorstore.similarity_search_with_score(
            question,
            **kwargs
        )

        context = ""
        references = ""
        for doc, score in results:
            context += f"\n[Case {doc.metadata['id']}]\n{doc.page_content}\n"
            references += f"- {doc.metadata['id']} (similarity: {1-score:.2f})\n"

        prompt = ChatPromptTemplate.from_messages([("human", PROMPT_TEMPLATE)])
        chain = prompt | llm

        response = chain.invoke({
            "question": question,
            "sw_version": sw_version or "Unknown",
            "model_type": model_type or "Unknown",
            "components": ", ".join(components) if components else "Unknown",
            "country": country or "Unknown",
            "context": context,
            "references": references
        })

        return response