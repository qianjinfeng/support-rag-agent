# rag_engine.py
import chromadb
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_community.llms import Tongyi
import os

# åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ï¼ˆä¸­æ–‡æ¨è BGEï¼‰
embedding_model = HuggingFaceEmbeddings(model_name="BAAI/bge-large-zh-v1.5")

# é€šä¹‰åƒé—® API
os.environ["DASHSCOPE_API_KEY"] = os.getenv("DASHSCOPE_API_KEY")

# Prompt æ¨¡æ¿
PROMPT_TEMPLATE = """
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŠ€æœ¯æ”¯æŒåŠ©æ‰‹ï¼Œè¯·æ ¹æ®ä»¥ä¸‹å†å²æ¡ˆä¾‹ï¼Œä¸ºå½“å‰é—®é¢˜æä¾›åˆ†æå’Œå»ºè®®ã€‚

ã€å½“å‰è®¾å¤‡ä¿¡æ¯ã€‘
- è½¯ä»¶ç‰ˆæœ¬: {sw_version}
- æœºå‹: {model_type}
- ç»„ä»¶: {components}
- å›½å®¶: {country}

ã€ç›¸ä¼¼å†å²æ¡ˆä¾‹ã€‘
{context}

ã€ç”¨æˆ·æè¿°çš„æ–°é—®é¢˜ã€‘
"{question}"

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼å›ç­”ï¼š

ğŸ” **é—®é¢˜åˆ†æ**ï¼š
ç®€è¦åˆ†æå¯èƒ½çš„æ ¹æœ¬åŸå› ã€‚

ğŸ› ï¸ **è§£å†³æ–¹æ¡ˆå»ºè®®**ï¼š
1. ...
2. ...

ğŸ“Œ **æ³¨æ„äº‹é¡¹**ï¼š
- å¦‚æ¶‰åŠå‡çº§/çƒ§å½•ï¼Œè¯·æé†’å¤‡ä»½ã€‚
- å¦‚ä¸å›½å®¶åˆè§„ç›¸å…³ï¼Œè¯·è¯´æ˜ã€‚

ğŸ”— **å‚è€ƒæ¡ˆä¾‹**ï¼š
{references}

â“ **å¦‚ä¿¡æ¯ä¸è¶³**ï¼š
è¯·å»ºè®®è¿›ä¸€æ­¥æ”¶é›†å“ªäº›ä¿¡æ¯ã€‚
"""

class RAGEngine:
    def __init__(self, db_path="./chroma_db"):
        self.db_path = db_path
        self.client = chromadb.PersistentClient(path=db_path)
        self.collection = None
        self.llm = Tongyi(model_name="qwen-max", api_key=os.getenv("DASHSCOPE_API_KEY"))
        self.vectorstore = None

    def load_data(self, data):
        """å¯¼å…¥å†å²é—®é¢˜æ•°æ®"""
        texts = []
        metadatas = []
        ids = []

        for item in data:
            text = f"{item['problem_description']} {item['root_cause']} {item['solution']}"
            texts.append(text)
            metadatas.append(item["metadata"])
            ids.append(item["id"])

        # åˆ›å»º Chroma å‘é‡åº“
        self.vectorstore = Chroma.from_texts(
            texts=texts,
            embedding=embedding_model,
            metadatas=metadatas,
            ids=ids,
            persist_directory=self.db_path
        )
        print(f"âœ… å·²å¯¼å…¥ {len(data)} æ¡é—®é¢˜åˆ°å‘é‡æ•°æ®åº“")

    def query(self, question, sw_version=None, model_type=None, components=None, country=None):
        """æŸ¥è¯¢ç›¸ä¼¼é—®é¢˜å¹¶ç”Ÿæˆå›ç­”"""
        if not self.vectorstore:
            self.vectorstore = Chroma(
                persist_directory=self.db_path,
                embedding_function=embedding_model
            )

        # æ„å»ºè¿‡æ»¤æ¡ä»¶
        where = {}
        if model_type:
            where["model_type"] = model_type
        if sw_version:
            where["sw_version"] = sw_version

        # æ£€ç´¢ç›¸ä¼¼æ¡ˆä¾‹ï¼ˆæœ€å¤š3ä¸ªï¼‰
        results = self.vectorstore.similarity_search_with_score(
            question, 
            k=3, 
            where=where
        )

        # æ„å»ºä¸Šä¸‹æ–‡
        context = ""
        references = ""
        for doc, score in results:
            context += f"\nã€æ¡ˆä¾‹ {doc.metadata['id']}ã€‘\né—®é¢˜: {doc.page_content}\n"
            references += f"- {doc.metadata['id']} (ç›¸ä¼¼åº¦: {1-score:.2f})\n"

        # ç”Ÿæˆ Prompt
        prompt = ChatPromptTemplate.from_messages([
            ("human", PROMPT_TEMPLATE)
        ])
        chain = prompt | self.llm

        response = chain.invoke({
            "question": question,
            "sw_version": sw_version or "æœªçŸ¥",
            "model_type": model_type or "æœªçŸ¥",
            "components": ", ".join(components) if components else "æœªçŸ¥",
            "country": country or "æœªçŸ¥",
            "context": context,
            "references": references
        })

        return response